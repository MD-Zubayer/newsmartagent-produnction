# aiAgent/gemini.py
from google import genai
from google.genai import types
from django.conf import settings
from aiAgent.utils import count_gemini_tokens



client = genai.Client(api_key=settings.GEMINI_API_KEY)



def generate_reply(system_prompt, messages, agent_config):

    model_name = agent_config.ai_model if 'gemini' in agent_config.ai_model else 'models/gemini-2.0-flash'

    formatted_history = []

    # 1Ô∏è‚É£ Always push current prompt as USER message
    formatted_history.append({
        "role": "user",
        "parts": [{"text": system_prompt}]
    })

    # 2Ô∏è‚É£ Add previous chat history
    for m in messages:
        role = "model" if m["role"] == "assistant" else "user"
        formatted_history.append({
            "role": role,
            "parts": [{"text": m["content"]}]
        })

    # üö® Safety guard
    if not formatted_history:
        return {
            "reply": "System error. Empty prompt.",
            "total_tokens": 0,
            "status": "error"
        }

    safety_settings = [
        types.SafetySetting(category="HARM_CATEGORY_HARASSMENT", threshold="BLOCK_NONE"),
        types.SafetySetting(category="HARM_CATEGORY_HATE_SPEECH", threshold="BLOCK_NONE"),
        types.SafetySetting(category="HARM_CATEGORY_SEXUALLY_EXPLICIT", threshold="BLOCK_NONE"),
        types.SafetySetting(category="HARM_CATEGORY_DANGEROUS_CONTENT", threshold="BLOCK_NONE"),
    ]

    try:
        max_token = int(agent_config.max_tokens) if agent_config.max_tokens else 500
    except:
        max_token = 500

    try:
        response = client.models.generate_content(
            model=model_name,
            contents=formatted_history,
            config=types.GenerateContentConfig(
                temperature=agent_config.temperature or 0.7,
                max_output_tokens=max_token,
                safety_settings=safety_settings,
                candidate_count=1
            )
        )

        reply = response.text.strip() if response.text else ""

        input_tokens = response.usage_metadata.prompt_token_count or 0
        output_tokens = response.usage_metadata.candidates_token_count or 0
        total_tokens = input_tokens + output_tokens

        return {
            "reply": reply or "Sorry, I didn't understand.",
            "input_tokens": input_tokens,
            "output_tokens": output_tokens,
            "total_tokens": total_tokens,
            "model_name": model_name,
            "status": "success" if reply else "empty"
        }

    except Exception as e:
        print(f'Gemini API Error: {str(e)}')
        return {
            "reply": "The system is experiencing some problems, please try again later.",
            "total_tokens": 0,
            "status": "error",
            "error_message": str(e)
        }

            
def generate_quick_summary(raw_text):
    try:
        model_name = 'models/gemini-2.5-flash'

        prompt = f"""Summarize the following Facebook post in maximum 3 short sentences.
                Rules:
                - Keep all key points.
                - Keep total length under 60-80 words.
                - No opinion.
                - No extra explanation.
                - Simple and clear language.
                Post: {raw_text}"""

        response = client.models.generate_content(
            model=model_name,
            contents=[prompt],
            config=types.GenerateContentConfig(
                temperature=0.2,
                max_output_tokens=600
            )
        )

        if response.text:
            summary = response.text.strip()
            print(f"‚úÖ Summary Generated Successfully: {summary[:50]}...")
            return summary
        return None
    except Exception as e:
        print(f'Summary Generation Error: {str(e)}')
        return None



def generate_dashboard_help(user_query, page_context, chat_history=[]):
    """
    ‡¶°‡ßç‡¶Ø‡¶æ‡¶∂‡¶¨‡ßã‡¶∞‡ßç‡¶° ‡¶á‡¶â‡¶ú‡¶æ‡¶∞‡¶¶‡ßá‡¶∞ ‡¶™‡ßç‡¶∞‡¶∂‡ßç‡¶®‡ßá‡¶∞ ‡¶â‡¶§‡ßç‡¶§‡¶∞ ‡¶¶‡ßá‡¶ì‡ßü‡¶æ‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø ‡¶¨‡¶ø‡¶∂‡ßá‡¶∑ ‡¶´‡¶æ‡¶Ç‡¶∂‡¶®‡•§
    """
    model_name = 'models/gemini-2.5-flash' # ‡¶Ü‡¶™‡¶®‡¶æ‡¶∞ ‡¶≤‡¶ó‡ßá‡¶∞ ‡¶ï‡ßã‡¶ü‡¶æ ‡¶Ö‡¶®‡ßÅ‡¶Ø‡¶æ‡ßü‡ßÄ 1.5-flash ‡¶ì ‡¶¶‡¶ø‡¶§‡ßá ‡¶™‡¶æ‡¶∞‡ßá‡¶®
    
    # ‡¶∏‡¶ø‡¶∏‡ßç‡¶ü‡ßá‡¶Æ ‡¶™‡ßç‡¶∞‡¶Æ‡ßç‡¶™‡¶ü ‡¶Ø‡¶æ AI-‡¶ï‡ßá ‡¶Ü‡¶™‡¶®‡¶æ‡¶∞ ‡¶°‡ßç‡¶Ø‡¶æ‡¶∂‡¶¨‡ßã‡¶∞‡ßç‡¶° ‡¶¨‡¶ø‡¶∂‡ßá‡¶∑‡¶ú‡ßç‡¶û ‡¶¨‡¶æ‡¶®‡¶æ‡¶¨‡ßá
    system_prompt = f"""
Role: Smart Professional Assistant.
Context: {page_context}
Language: Bengali.

Instruction:
- ‡¶á‡¶â‡¶ú‡¶æ‡¶∞ ‡¶§‡¶æ‡¶∞ ‡¶è‡¶ï‡¶æ‡¶â‡¶®‡ßç‡¶ü ‡¶¨‡¶æ ‡¶°‡ßç‡¶Ø‡¶æ‡¶∂‡¶¨‡ßã‡¶∞‡ßç‡¶° ‡¶∏‡¶Æ‡ßç‡¶™‡¶∞‡ßç‡¶ï‡¶ø‡¶§ ‡¶§‡¶•‡ßç‡¶Ø ‡¶ú‡¶æ‡¶®‡¶§‡ßá ‡¶ö‡¶æ‡¶á‡¶≤‡ßá "‡¶á‡¶â‡¶ú‡¶æ‡¶∞‡ßá‡¶∞ ‡¶≤‡¶æ‡¶á‡¶≠ ‡¶°‡¶æ‡¶ü‡¶æ" ‡¶Ö‡¶Ç‡¶∂‡¶ü‡¶ø ‡¶•‡ßá‡¶ï‡ßá ‡¶â‡¶§‡ßç‡¶§‡¶∞ ‡¶¶‡¶ø‡¶®‡•§
- ‡¶≤‡¶æ‡¶á‡¶≠ ‡¶°‡¶æ‡¶ü‡¶æ‡¶§‡ßá ‡¶Ø‡¶æ ‡¶®‡ßá‡¶á, ‡¶∏‡ßá‡¶ü‡¶ø ‡¶®‡¶ø‡ßü‡ßá ‡¶Ö‡¶®‡ßÅ‡¶Æ‡¶æ‡¶® ‡¶ï‡¶∞‡ßá ‡¶ï‡ßã‡¶®‡ßã ‡¶∏‡¶Ç‡¶ñ‡ßç‡¶Ø‡¶æ ‡¶¨‡¶æ ‡¶§‡¶•‡ßç‡¶Ø ‡¶¶‡ßá‡¶¨‡ßá‡¶® ‡¶®‡¶æ‡•§
- ‡¶§‡¶•‡ßç‡¶Ø ‡¶®‡¶æ ‡¶•‡¶æ‡¶ï‡¶≤‡ßá ‡¶¨‡¶ø‡¶®‡ßÄ‡¶§‡¶≠‡¶æ‡¶¨‡ßá ‡¶¨‡¶≤‡ßÅ‡¶® ‡¶Ø‡ßá ‡¶Ü‡¶™‡¶®‡¶ø ‡¶è‡¶á ‡¶Æ‡ßÅ‡¶π‡ßÇ‡¶∞‡ßç‡¶§‡ßá ‡¶∏‡ßá‡¶ü‡¶ø ‡¶¶‡ßá‡¶ñ‡¶§‡ßá ‡¶™‡¶æ‡¶ö‡ßç‡¶õ‡ßá‡¶® ‡¶®‡¶æ‡•§
"""

    formatted_history = []
    
    # ‡ßß. ‡¶∏‡¶ø‡¶∏‡ßç‡¶ü‡ßá‡¶Æ ‡¶á‡¶®‡¶∏‡ßç‡¶ü‡ßç‡¶∞‡¶æ‡¶ï‡¶∂‡¶® ‡¶∏‡ßá‡¶ü ‡¶ï‡¶∞‡¶æ
    # ‡¶®‡¶§‡ßÅ‡¶® SDK-‡¶§‡ßá contents ‡¶è‡¶∞ ‡¶¨‡¶¶‡¶≤‡ßá config ‡¶è‡¶ì system_instruction ‡¶¶‡ßá‡¶ì‡ßü‡¶æ ‡¶Ø‡¶æ‡ßü
    
    # ‡ß®. ‡¶π‡¶ø‡¶∏‡ßç‡¶ü‡ßç‡¶∞‡¶ø ‡¶´‡¶∞‡¶Æ‡ßç‡¶Ø‡¶æ‡¶ü ‡¶ï‡¶∞‡¶æ
    for m in chat_history:
        role = "model" if m["role"] == "assistant" else "user"
        formatted_history.append({"role": role, "parts": [{"text": m["content"]}]})
    
    # ‡ß©. ‡¶¨‡¶∞‡ßç‡¶§‡¶Æ‡¶æ‡¶® ‡¶™‡ßç‡¶∞‡¶∂‡ßç‡¶® ‡¶Ø‡ßÅ‡¶ï‡ßç‡¶§ ‡¶ï‡¶∞‡¶æ
    formatted_history.append({"role": "user", "parts": [{"text": user_query}]})

    try:
        response = client.models.generate_content(
            model=model_name,
            contents=formatted_history,
            config=types.GenerateContentConfig(
                system_instruction=system_prompt, # ‡¶è‡¶ñ‡¶æ‡¶®‡ßá ‡¶á‡¶®‡¶∏‡ßç‡¶ü‡ßç‡¶∞‡¶æ‡¶ï‡¶∂‡¶® ‡¶¶‡ßá‡¶ì‡ßü‡¶æ ‡¶∏‡¶¨‡¶ö‡ßá‡ßü‡ßá ‡¶ï‡¶æ‡¶∞‡ßç‡¶Ø‡¶ï‡¶∞
                temperature=0.3, # ‡¶°‡ßç‡¶Ø‡¶æ‡¶∂‡¶¨‡ßã‡¶∞‡ßç‡¶° ‡¶π‡ßá‡¶≤‡ßç‡¶™‡ßá‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø ‡¶ï‡ßç‡¶∞‡¶ø‡ßü‡ßá‡¶ü‡¶ø‡¶≠‡¶ø‡¶ü‡¶ø ‡¶ï‡¶Æ, ‡¶è‡¶ï‡ßÅ‡¶∞‡ßá‡¶∏‡¶ø ‡¶¨‡ßá‡¶∂‡¶ø ‡¶¶‡¶∞‡¶ï‡¶æ‡¶∞
                max_output_tokens=200,
            )
        )

        reply = response.text.strip() if response.text else ""
        
        return {
            "reply": reply or "‡¶¶‡ßÅ‡¶É‡¶ñ‡¶ø‡¶§, ‡¶Ü‡¶Æ‡¶ø ‡¶¨‡ßÅ‡¶ù‡¶§‡ßá ‡¶™‡¶æ‡¶∞‡¶õ‡¶ø ‡¶®‡¶æ‡•§",
            "total_tokens": response.usage_metadata.total_token_count or 0,
            "status": "success"
        }

    except Exception as e:
        print(f'Dashboard AI Error: {str(e)}')
        return {
            "reply": "‡¶∏‡¶æ‡¶∞‡ßç‡¶≠‡¶æ‡¶∞‡ßá ‡¶∏‡¶Æ‡¶∏‡ßç‡¶Ø‡¶æ ‡¶π‡¶ö‡ßç‡¶õ‡ßá, ‡¶Ö‡¶®‡ßÅ‡¶ó‡ßç‡¶∞‡¶π ‡¶ï‡¶∞‡ßá ‡¶è‡¶ï‡¶ü‡ßÅ ‡¶™‡¶∞‡ßá ‡¶ö‡ßá‡¶∑‡ßç‡¶ü‡¶æ ‡¶ï‡¶∞‡ßÅ‡¶®‡•§",
            "status": "error"
        }